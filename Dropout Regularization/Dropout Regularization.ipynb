{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:04.081081Z",
     "start_time": "2020-10-17T06:29:04.065121Z"
    }
   },
   "outputs": [],
   "source": [
    "# To solve the isuue of overfitting and underfitting we use Regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='fitting.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:04.097035Z",
     "start_time": "2020-10-17T06:29:04.082077Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropout Regularization is one of the techniques used in Deep Learning to tackle Overfitting of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:04.112993Z",
     "start_time": "2020-10-17T06:29:04.099030Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you are using a deep neural network (i.e., having too many hidden layers) or may be you are having so many epochs,\n",
    "# your neural network tends to overfit the model and now it cannot perform well on test dataset as now it cannot generalize well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle with the overfitting problem you can drop some of the neurons. The number of neurons dropped can be random and from different hidden layers.<br><img src='drop.jpg' width=500><br>\n",
    "In this image, the neurons in pink are being dropped from the network. You can also observe that from each hidden layer we have dropped 50% of the neurons. There is a factor in dropout regularization where you specify the percent of neurons to be dropped. In this case, it would be 0.5 (i.e., 50%).<br>\n",
    "Now that you've dropped some of your neurons, you do the first forward pass and calculate the error. When it comes to the second forward pass, you again randomly remove some of the neurons from the original structure and perform the second forward pass. You repeat the same process.<br><img src='drop2.jpg' width=500><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:04.128950Z",
     "start_time": "2020-10-17T06:29:04.113990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note that the dropout rate (2nd sample) is still the same as that of the first sample.\n",
    "# You can also change the dropout rate for the second layer in contrary to the first layer.\n",
    "# For example, you can have dropout rate for the first layer to be 50% and that of the second layer to be 25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='a.jpg' width=500><br>\n",
    "1. Since you've dropped few of your neurons, now your model cannot rely totally on the neurons. This is because you are dropping your neurons at random and hence backtracking occurs on different parameters. Also now your ***model isn't biased to the prominent features.*** This is because there are chances that the neurons affecting the prominent features are dropped.<br>\n",
    "2. Neurons will also not learn redundant details of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:06.829888Z",
     "start_time": "2020-10-17T06:29:04.129947Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:06.861795Z",
     "start_time": "2020-10-17T06:29:06.830853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.0790</td>\n",
       "      <td>0.0707</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.5664</td>\n",
       "      <td>0.6609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.0587</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.1268</td>\n",
       "      <td>0.1498</td>\n",
       "      <td>0.1436</td>\n",
       "      <td>0.0561</td>\n",
       "      <td>0.0832</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>0.2352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0257</td>\n",
       "      <td>0.0447</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.1323</td>\n",
       "      <td>0.1608</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.0847</td>\n",
       "      <td>0.0561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>0.1326</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.2984</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "133  0.0790  0.0707  0.0352  0.1660  0.1330  0.0226  0.0771  0.2678  0.5664   \n",
       "102  0.0587  0.1210  0.1268  0.1498  0.1436  0.0561  0.0832  0.0672  0.1372   \n",
       "44   0.0257  0.0447  0.0388  0.0239  0.1315  0.1323  0.1608  0.2145  0.0847   \n",
       "152  0.0131  0.0201  0.0045  0.0217  0.0230  0.0481  0.0742  0.0333  0.1369   \n",
       "175  0.0294  0.0123  0.0117  0.0113  0.0497  0.0998  0.1326  0.1117  0.2984   \n",
       "\n",
       "         9   ...      51      52      53      54      55      56      57  \\\n",
       "133  0.6609  ...  0.0298  0.0390  0.0294  0.0175  0.0249  0.0141  0.0073   \n",
       "102  0.2352  ...  0.0331  0.0111  0.0088  0.0158  0.0122  0.0038  0.0101   \n",
       "44   0.0561  ...  0.0096  0.0153  0.0096  0.0131  0.0198  0.0025  0.0199   \n",
       "152  0.2079  ...  0.0168  0.0086  0.0045  0.0062  0.0065  0.0030  0.0066   \n",
       "175  0.3473  ...  0.0056  0.0104  0.0079  0.0014  0.0054  0.0015  0.0006   \n",
       "\n",
       "         58      59  60  \n",
       "133  0.0025  0.0101   M  \n",
       "102  0.0228  0.0124   M  \n",
       "44   0.0255  0.0180   R  \n",
       "152  0.0029  0.0053   M  \n",
       "175  0.0081  0.0043   M  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df=pd.read_csv('sonar_dataset.csv',header=None) # Since, we do not have a header in this csv file, we inititalize header to none\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:06.877760Z",
     "start_time": "2020-10-17T06:29:06.862768Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here, R ==> Rock and M ==> Mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:06.894683Z",
     "start_time": "2020-10-17T06:29:06.882715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:06.909644Z",
     "start_time": "2020-10-17T06:29:06.897679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:06.925626Z",
     "start_time": "2020-10-17T06:29:06.911639Z"
    }
   },
   "outputs": [],
   "source": [
    "# We don't have any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:06.941558Z",
     "start_time": "2020-10-17T06:29:06.926598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    111\n",
       "R     97\n",
       "Name: 60, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets count the number of classes in the output;\n",
    "df[60].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:06.956552Z",
     "start_time": "2020-10-17T06:29:06.943553Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can see that our dataset is not likely to be imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:06.971510Z",
     "start_time": "2020-10-17T06:29:06.957546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: X ==> (208, 60)  y ==> (208,)\n"
     ]
    }
   ],
   "source": [
    "# Lets split our dataset into X and y;\n",
    "X=df.drop(columns=60)\n",
    "y=df[60]\n",
    "print(f'shape: X ==> {X.shape}  y ==> {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:06.986486Z",
     "start_time": "2020-10-17T06:29:06.972507Z"
    }
   },
   "outputs": [],
   "source": [
    "# We need to convert the y series into one hot encoded series.\n",
    "y=pd.get_dummies(y,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:07.001448Z",
     "start_time": "2020-10-17T06:29:06.987482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     R\n",
       "116  0\n",
       "169  0\n",
       "59   1\n",
       "67   1\n",
       "104  0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:07.017387Z",
     "start_time": "2020-10-17T06:29:07.003406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R\n",
       "0    111\n",
       "1     97\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:07.032350Z",
     "start_time": "2020-10-17T06:29:07.019384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 60 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       208 non-null    float64\n",
      " 1   1       208 non-null    float64\n",
      " 2   2       208 non-null    float64\n",
      " 3   3       208 non-null    float64\n",
      " 4   4       208 non-null    float64\n",
      " 5   5       208 non-null    float64\n",
      " 6   6       208 non-null    float64\n",
      " 7   7       208 non-null    float64\n",
      " 8   8       208 non-null    float64\n",
      " 9   9       208 non-null    float64\n",
      " 10  10      208 non-null    float64\n",
      " 11  11      208 non-null    float64\n",
      " 12  12      208 non-null    float64\n",
      " 13  13      208 non-null    float64\n",
      " 14  14      208 non-null    float64\n",
      " 15  15      208 non-null    float64\n",
      " 16  16      208 non-null    float64\n",
      " 17  17      208 non-null    float64\n",
      " 18  18      208 non-null    float64\n",
      " 19  19      208 non-null    float64\n",
      " 20  20      208 non-null    float64\n",
      " 21  21      208 non-null    float64\n",
      " 22  22      208 non-null    float64\n",
      " 23  23      208 non-null    float64\n",
      " 24  24      208 non-null    float64\n",
      " 25  25      208 non-null    float64\n",
      " 26  26      208 non-null    float64\n",
      " 27  27      208 non-null    float64\n",
      " 28  28      208 non-null    float64\n",
      " 29  29      208 non-null    float64\n",
      " 30  30      208 non-null    float64\n",
      " 31  31      208 non-null    float64\n",
      " 32  32      208 non-null    float64\n",
      " 33  33      208 non-null    float64\n",
      " 34  34      208 non-null    float64\n",
      " 35  35      208 non-null    float64\n",
      " 36  36      208 non-null    float64\n",
      " 37  37      208 non-null    float64\n",
      " 38  38      208 non-null    float64\n",
      " 39  39      208 non-null    float64\n",
      " 40  40      208 non-null    float64\n",
      " 41  41      208 non-null    float64\n",
      " 42  42      208 non-null    float64\n",
      " 43  43      208 non-null    float64\n",
      " 44  44      208 non-null    float64\n",
      " 45  45      208 non-null    float64\n",
      " 46  46      208 non-null    float64\n",
      " 47  47      208 non-null    float64\n",
      " 48  48      208 non-null    float64\n",
      " 49  49      208 non-null    float64\n",
      " 50  50      208 non-null    float64\n",
      " 51  51      208 non-null    float64\n",
      " 52  52      208 non-null    float64\n",
      " 53  53      208 non-null    float64\n",
      " 54  54      208 non-null    float64\n",
      " 55  55      208 non-null    float64\n",
      " 56  56      208 non-null    float64\n",
      " 57  57      208 non-null    float64\n",
      " 58  58      208 non-null    float64\n",
      " 59  59      208 non-null    float64\n",
      "dtypes: float64(60)\n",
      "memory usage: 97.6 KB\n"
     ]
    }
   ],
   "source": [
    "# You can also change the dtypes of the dataset in order to save memory. For example: int64 can be converted into uint8 \n",
    "# and float64 into float32.\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:07.048306Z",
     "start_time": "2020-10-17T06:29:07.033347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 60 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       208 non-null    float32\n",
      " 1   1       208 non-null    float32\n",
      " 2   2       208 non-null    float32\n",
      " 3   3       208 non-null    float32\n",
      " 4   4       208 non-null    float32\n",
      " 5   5       208 non-null    float32\n",
      " 6   6       208 non-null    float32\n",
      " 7   7       208 non-null    float32\n",
      " 8   8       208 non-null    float32\n",
      " 9   9       208 non-null    float32\n",
      " 10  10      208 non-null    float32\n",
      " 11  11      208 non-null    float32\n",
      " 12  12      208 non-null    float32\n",
      " 13  13      208 non-null    float32\n",
      " 14  14      208 non-null    float32\n",
      " 15  15      208 non-null    float32\n",
      " 16  16      208 non-null    float32\n",
      " 17  17      208 non-null    float32\n",
      " 18  18      208 non-null    float32\n",
      " 19  19      208 non-null    float32\n",
      " 20  20      208 non-null    float32\n",
      " 21  21      208 non-null    float32\n",
      " 22  22      208 non-null    float32\n",
      " 23  23      208 non-null    float32\n",
      " 24  24      208 non-null    float32\n",
      " 25  25      208 non-null    float32\n",
      " 26  26      208 non-null    float32\n",
      " 27  27      208 non-null    float32\n",
      " 28  28      208 non-null    float32\n",
      " 29  29      208 non-null    float32\n",
      " 30  30      208 non-null    float32\n",
      " 31  31      208 non-null    float32\n",
      " 32  32      208 non-null    float32\n",
      " 33  33      208 non-null    float32\n",
      " 34  34      208 non-null    float32\n",
      " 35  35      208 non-null    float32\n",
      " 36  36      208 non-null    float32\n",
      " 37  37      208 non-null    float32\n",
      " 38  38      208 non-null    float32\n",
      " 39  39      208 non-null    float32\n",
      " 40  40      208 non-null    float32\n",
      " 41  41      208 non-null    float32\n",
      " 42  42      208 non-null    float32\n",
      " 43  43      208 non-null    float32\n",
      " 44  44      208 non-null    float32\n",
      " 45  45      208 non-null    float32\n",
      " 46  46      208 non-null    float32\n",
      " 47  47      208 non-null    float32\n",
      " 48  48      208 non-null    float32\n",
      " 49  49      208 non-null    float32\n",
      " 50  50      208 non-null    float32\n",
      " 51  51      208 non-null    float32\n",
      " 52  52      208 non-null    float32\n",
      " 53  53      208 non-null    float32\n",
      " 54  54      208 non-null    float32\n",
      " 55  55      208 non-null    float32\n",
      " 56  56      208 non-null    float32\n",
      " 57  57      208 non-null    float32\n",
      " 58  58      208 non-null    float32\n",
      " 59  59      208 non-null    float32\n",
      "dtypes: float32(60)\n",
      "memory usage: 48.9 KB\n"
     ]
    }
   ],
   "source": [
    "X=X.astype('float32')\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:07.063272Z",
     "start_time": "2020-10-17T06:29:07.050302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   R       208 non-null    uint8\n",
      "dtypes: uint8(1)\n",
      "memory usage: 336.0 bytes\n"
     ]
    }
   ],
   "source": [
    "# You can see that memory usage has been reduced to 50%. Similarly we check for the y series\n",
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:07.093245Z",
     "start_time": "2020-10-17T06:29:07.066260Z"
    }
   },
   "outputs": [],
   "source": [
    "# It is already in its lowest dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:07.108509Z",
     "start_time": "2020-10-17T06:29:07.094216Z"
    }
   },
   "outputs": [],
   "source": [
    "# There was no need of reducing the space in this dataset as the memory was only about 100 KB. But there are cases where\n",
    "# the memory usage will be about 100 MB and more and hence reducing the memory usage is a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:07.124234Z",
     "start_time": "2020-10-17T06:29:07.109508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now we divide the dataset into train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:07.202091Z",
     "start_time": "2020-10-17T06:29:07.125215Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:07.217318Z",
     "start_time": "2020-10-17T06:29:07.203055Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:14.363987Z",
     "start_time": "2020-10-17T06:29:07.218317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 156 samples\n",
      "Epoch 1/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6902 - accuracy: 0.4872\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 352us/sample - loss: 0.6715 - accuracy: 0.5705\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 358us/sample - loss: 0.6503 - accuracy: 0.6410\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 364us/sample - loss: 0.6140 - accuracy: 0.7436\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 377us/sample - loss: 0.5883 - accuracy: 0.6987\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 350us/sample - loss: 0.5664 - accuracy: 0.7051\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 356us/sample - loss: 0.5490 - accuracy: 0.7692\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 384us/sample - loss: 0.5158 - accuracy: 0.7821\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 377us/sample - loss: 0.4736 - accuracy: 0.7885\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 339us/sample - loss: 0.4585 - accuracy: 0.8013\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 352us/sample - loss: 0.4261 - accuracy: 0.8333\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 410us/sample - loss: 0.4029 - accuracy: 0.8526\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 342us/sample - loss: 0.3714 - accuracy: 0.8526\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 352us/sample - loss: 0.3708 - accuracy: 0.8205\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 371us/sample - loss: 0.3388 - accuracy: 0.8974\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 343us/sample - loss: 0.3498 - accuracy: 0.8590\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 339us/sample - loss: 0.3079 - accuracy: 0.8590\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 339us/sample - loss: 0.2909 - accuracy: 0.8910\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 352us/sample - loss: 0.2798 - accuracy: 0.8846\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 339us/sample - loss: 0.2860 - accuracy: 0.8590\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 339us/sample - loss: 0.2969 - accuracy: 0.8846\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 409us/sample - loss: 0.2496 - accuracy: 0.9038\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 358us/sample - loss: 0.2289 - accuracy: 0.9231\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 320us/sample - loss: 0.2130 - accuracy: 0.9103\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 307us/sample - loss: 0.2005 - accuracy: 0.9359\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 275us/sample - loss: 0.2121 - accuracy: 0.9423\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 269us/sample - loss: 0.2090 - accuracy: 0.9231\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 269us/sample - loss: 0.2087 - accuracy: 0.9038\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 281us/sample - loss: 0.1869 - accuracy: 0.9295\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 384us/sample - loss: 0.1657 - accuracy: 0.9295\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 314us/sample - loss: 0.1570 - accuracy: 0.9551\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 326us/sample - loss: 0.1421 - accuracy: 0.9615\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 396us/sample - loss: 0.1401 - accuracy: 0.9423\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 384us/sample - loss: 0.1493 - accuracy: 0.9615\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 320us/sample - loss: 0.1458 - accuracy: 0.9487\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 320us/sample - loss: 0.1062 - accuracy: 0.9744\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 321us/sample - loss: 0.1386 - accuracy: 0.9359\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 299us/sample - loss: 0.1151 - accuracy: 0.9679\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 294us/sample - loss: 0.0979 - accuracy: 0.9808\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 313us/sample - loss: 0.0970 - accuracy: 0.9808\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 320us/sample - loss: 0.0819 - accuracy: 0.9872\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 364us/sample - loss: 0.0874 - accuracy: 0.9936\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 326us/sample - loss: 0.0755 - accuracy: 0.9872\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 326us/sample - loss: 0.0732 - accuracy: 0.9808\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 409us/sample - loss: 0.0775 - accuracy: 0.9744\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 377us/sample - loss: 0.0598 - accuracy: 0.9936\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 339us/sample - loss: 0.0512 - accuracy: 0.9872\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 320us/sample - loss: 0.0427 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 391us/sample - loss: 0.0484 - accuracy: 0.9936\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 326us/sample - loss: 0.0433 - accuracy: 0.9936\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 320us/sample - loss: 0.0414 - accuracy: 0.9936\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 326us/sample - loss: 0.0390 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 269us/sample - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 313us/sample - loss: 0.0422 - accuracy: 0.9936\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 262us/sample - loss: 0.0311 - accuracy: 0.9936\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 269us/sample - loss: 0.0257 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 300us/sample - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 294us/sample - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 275us/sample - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 281us/sample - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 262us/sample - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 288us/sample - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 281us/sample - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 275us/sample - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 256us/sample - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 294us/sample - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 281us/sample - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 249us/sample - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 269us/sample - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 356us/sample - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 281us/sample - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 261us/sample - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 256us/sample - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 281us/sample - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 256us/sample - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 275us/sample - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 275us/sample - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 262us/sample - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 262us/sample - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 256us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 262us/sample - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 262us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 269us/sample - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 256us/sample - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 256us/sample - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 269us/sample - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 249us/sample - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 288us/sample - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 262us/sample - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 294us/sample - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 343us/sample - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 454us/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 381us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 323us/sample - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 325us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 294us/sample - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 269us/sample - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 249us/sample - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 288us/sample - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 269us/sample - loss: 0.0022 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1de03e4c9c8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=keras.Sequential([\n",
    "    keras.layers.Dense(60,input_dim=60,activation='relu'), # We can use input_dim instead of input_shape=(n,). Input to hidden\n",
    "    keras.layers.Dense(30,activation='relu'), # Hidden to Hidden\n",
    "    keras.layers.Dense(15,activation='relu'), # Hidden to Hidden\n",
    "    keras.layers.Dense(1,activation='sigmoid') # Hidden to Output\n",
    "])\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train, epochs=100, batch_size=8) # We specified batch_size because we are performing kind off mini batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:14.474346Z",
     "start_time": "2020-10-17T06:29:14.368021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "52/52 [==============================] - 0s 424us/sample - loss: 0.9161 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9160669766939603, 0.75]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can see that our model is overfit, lets calculate the score\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:14.489273Z",
     "start_time": "2020-10-17T06:29:14.481292Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can see that our model is only 77% accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:14.553081Z",
     "start_time": "2020-10-17T06:29:14.491267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(52, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets predict the values\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:14.568043Z",
     "start_time": "2020-10-17T06:29:14.555075Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred=y_pred.reshape(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:14.583134Z",
     "start_time": "2020-10-17T06:29:14.569038Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred=y_pred.round() # Since sigmoid gives output b/w 0 and 1, we round the values to get the answer (i.e., > 0.5==1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:14.599051Z",
     "start_time": "2020-10-17T06:29:14.587113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0.], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:14.615011Z",
     "start_time": "2020-10-17T06:29:14.602039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_test).reshape(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:14.630964Z",
     "start_time": "2020-10-17T06:29:14.619992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets also print the classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:14.646922Z",
     "start_time": "2020-10-17T06:29:14.634953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.85      0.78        27\n",
      "           1       0.80      0.64      0.71        25\n",
      "\n",
      "    accuracy                           0.75        52\n",
      "   macro avg       0.76      0.75      0.75        52\n",
      "weighted avg       0.76      0.75      0.75        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:14.931088Z",
     "start_time": "2020-10-17T06:29:14.649912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(33.0, 0.5, 'Y_true')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEHCAYAAABcCaZFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYC0lEQVR4nO3dfbxVVZ3H8c8XRKsrJmaSPKSZhJGZOoSVOhODMkAWTdMY1BQ6OFcbH5um0Cgte3J6cKppRueO4UMqVq9iMkOUqBkGR0x0HMXAULKBK4KKAYIG957f/HH2oeP13HP2vZxz9zmb79vXep2z135YP3zRz9U6a62tiMDMzFrboKwDMDOzPedkbmaWA07mZmY54GRuZpYDTuZmZjmwT9YBVLPr6bWeamMv8fIRJ2cdgjWhrp2d2tNn9CXnDDn4iD1ur57cMzczy4Gm7pmbmQ2oQnfWEfSbk7mZWUl3V10eI2k0cAMwHAigIyK+KemrwLuBncBjwJkR8bsK9z8ObAO6ga6IGF+rTQ+zmJklIgqpSw1dwMcjYhzwNuBcSeOAxcDREXEM8GvgkirPmBgRx6ZJ5OCeuZnZHxRqJulUImIDsCH5vk3SKmBkRNxZdtly4P11aRD3zM3M/iAKqYukdkkrykp7pUdKOhw4Drinx6m/Bm7vLRLgTkn39fbcntwzNzMr6cMPoBHRAXRUu0bS/sAPgYsiYmtZ/VyKQzE39XLrSRHRKekQYLGk1RGxtFpb7pmbmZX0oWdei6QhFBP5TRHxo7L6M4DTgA9FL9vWRkRn8rkJWABMqNWee+ZmZomo32wWAd8BVkXElWX1U4BPAn8SETt6ubcNGJSMtbcBk4HLa7XpZG5mVlKnH0CBE4EPAw9JeiCp+xTwLWA/ikMnAMsj4hxJI4BrImIaxemMC5Lz+wA3R8SiWg06mZuZlaQYPkn1mIhlQKXl/gt7uf4JYFryfS3wlr626WRuZlbiFaBmZjlQp555FpzMzcxK6jdmPuCczM3MSuo0myULTuZmZokIj5mbmbU+j5mbmeWAx8zNzHLAPXMzsxzo3pV1BP3mZG5mVuJhFjOzHPAwi5lZDrhnbmaWA07mZmatz4uGzMzywMv5zcxywMMsZmY54NksZmY54J65mVkOuGduZpYDLdwzH5R1AGZmTaO7K32pQtJoSb+Q9CtJD0u6MKk/SNJiSWuSz2G93D8ruWaNpFlpQncyNzMrKRTSl+q6gI9HxDjgbcC5ksYBFwNLImIMsCQ5fhFJBwGXAScAE4DLekv65ZzMzcxKopC+VHtMxIaIuD/5vg1YBYwEpgPXJ5ddD7y3wu1/BiyOiM0R8SywGJhSK3SPmZuZlfRhzFxSO9BeVtURER0VrjscOA64BxgeERuSU08Cwys8eiSwrux4fVJXlZO5mVlJH2azJIn7Jcm7nKT9gR8CF0XEVknl94ek6GekL+FhFjOzkq6u9KUGSUMoJvKbIuJHSfVGSYcm5w8FNlW4tRMYXXY8KqmrysnczKwkIn2pQsUu+HeAVRFxZdmpW4HS7JRZwI8r3H4HMFnSsOSHz8lJXVUeZjEzK6nfPPMTgQ8DD0l6IKn7FHAF8H1Js4HfAqcDSBoPnBMRZ0XEZkmfB+5N7rs8IjbXatDJ3MyspE7JPCKWAerl9KQK168Azio7ngfM60ubTuZmZiVezm9mlgMtvJzfydzMrKTbbxoyM2t97pmbmeWAx8zNzFpfFOq2IHPAOZmbmZV4mMXMLAc8zGJmlgNdns1iZtb6PMxi9bRh41N86vNf45lnn0WI90+fyodPfy//1HEDP192N4M0iIOGvZIvzv04h7z6VVmHaxkZNGgQ9yy/nSc6n2T6n6d6s5jVUmMDrWbmZN6E9hk8mE+c/zeMG3sk27fv4PTZF/COtx7HmR/6C85v/wgAN/7gx1x17c1c9snzM47WsnLB+WexevUaDhg6NOtQ8qOFe+YN3QJX0lGS5kj6VlLmSHpjI9vMg1cffBDjxh4JQFvbKzjisNFsfOoZ9m9r233N88+/gHrbxsdyb+TIQ5k2dRLz5s3POpR8KUT60mQa1jOXNAeYCdwC/DKpHgXMl3RLRFzRqLbzpHPDRlateYxj3jQWgG/+63XcumgJQ9vamPdP/le4t7ry65/j4ku+wNCh+2cdSr608HL+RvbMZwNvjYgrIuLGpFxB8W3Ts3u7SVK7pBWSVlxzw97d69ix43k+NvcLzLng7N298gvPPoMlC77LuyZP5OYf/iTjCC0L75p2Cps2Pc39//NQ1qHkThQKqUuzaWQyLwAjKtQfmpyrKCI6ImJ8RIw/6yMzGxZcs9vV1cVFc7/AuyZP5NR3nviS86dNnsjP/uOuDCKzrL3jHeN592mTefTXy7npxn9h4sQTuf66b2UdVj54mKWii4AlktbwhzdNvxY4Ejivge22vIjg0i9/gyMOG82sGe/bXf/bdZ0cNrr4ku6f/9fdvO6wUVmFaBma++krmPvp4hDbn/zx2/m7j53DrDMuyDiqnPCioZeKiEWS3kBxWGVkUt0J3BsRrTswNQD+58GH+cmiJYx5/eH8xaxzAbjw7Fn86LY7efz/1qNBYsRrDuHST3gmi1ldNWGPOy1FE8+r3PX02uYNzjLz8hEnZx2CNaGunZ17PL9r+2dnps45bZ+d31TzyTzP3MyspI6zWSTNA04DNkXE0Und94CxySUHAr+LiGMr3Ps4sA3oBroiYnyt9pzMzcxK6jvMch3wbeCGUkVEfKD0XdLXgS1V7p8YEU+nbczJ3MwsUc8phxGxVNLhlc5JEnA68Kf1aq+hK0DNzFpKH6Ymlq+JSUp7H1o6GdgYEWt6OR/AnZLuS/tc98zNzEr6MMwSER1ARz9bmglUWxV5UkR0SjoEWCxpdUQsrfZAJ3Mzs5IBmGcuaR/gfcAf9RpGRGfyuUnSAopTvKsmcw+zmJkloquQuuyBU4DVEbG+0klJbZKGlr4Dk4GVtR7qZG5mVlLH5fyS5gN3A2MlrZdU2pNqBj2GWCSNkLQwORwOLJP0vxQ3KfxpRCyq1Z6HWczMSuo7m6Xi5lIRcUaFuieAacn3tcBb+tqek7mZWUkLL+d3MjczK3EyNzNrfdHtXRPNzFqfe+ZmZq0vnMzNzHLAydzMLAdad8jcydzMrMTDLGZmedDlZG5m1vLcMzczywOPmZuZtT73zM3M8sA9czOz1hddWUfQf07mZmaJAXjRUMM4mZuZlTiZm5m1PvfMzcxywMnczCwHnMzNzHIgupV1CP02KOsAzMyaRRSUutQiaZ6kTZJWltV9VlKnpAeSMq2Xe6dIekTSo5IuThO7k7mZWSIK6UsK1wFTKtT/Y0Qcm5SFPU9KGgz8MzAVGAfMlDSuVmNO5mZmiQilLrWfFUuBzf0IYwLwaESsjYidwC3A9Fo3OZmbmSX60jOX1C5pRVlpT9nMeZIeTIZhhlU4PxJYV3a8PqmrysnczCzRlzHziOiIiPFlpSNFE1cBrweOBTYAX69X7J7NYmaWKDR4NktEbCx9l/RvwG0VLusERpcdj0rqqnLP3MwsUc/ZLJVIOrTs8M+BlRUuuxcYI+l1kvYFZgC31np2qmQu6Q2SlpSm2Eg6RtKn09xrZtYqItKXWiTNB+4GxkpaL2k28BVJD0l6EJgIfCy5doSkhcUYogs4D7gDWAV8PyIertlepIhK0n8CnwD+NSKOS+pWRsTRtf9I/bfr6bWtu1O8NczLR5ycdQjWhLp2du7xGMnaN09OnXOOeOjOplphlHbM/BUR8UvpRbG38M6/ZmYvlWbKYbNKm8yflvR6IAAkvZ/iL7FmZrnR3cLL+dMm83OBDuAoSZ3Ab4C/alhUZmYZyH3PPCLWAqdIagMGRcS2xoZlZjbw+jtLpRmkSuaSLu1xDEBEXN6AmMzMMpFmlkqzSjvMsr3s+8uA0yhOmTEzy43c98wj4kVLTiV9jeIcSDOz3Cjkfcy8gldQXGJqZpYbhbz3zCU9RDItERgMvBrweLmZ5cre0DM/rex7F7AxWXJqZpYbuZ6amLz14o6IOGoA4jEzy0yuZ7NERHfyLrrXRsT/DURQJeeNnzOQzVmL2DT1yKxDsJzaG4ZZhgEPS/olZdMUI+I9DYnKzCwDuR5mSXymoVGYmTWB7r0gmU+LiBeNeUj6B+A/6x+SmVk2WnmYJe2bhk6tUDe1noGYmWUtQqlLs6naM5f0UeBvgSOSN2OUDAXuamRgZmYDrZB1AHug1jDLzcDtwJeBi8vqt0XE5tKBpGER8WwD4jMzGzBB8/W406qazCNiC7AFmFnjOUuA4+sVlJlZFrqacPgkrbRj5rW07r8BM7NEoNSlFknzJG2StLKs7quSVkt6UNICSQf2cu/jyYufH5C0Ik3s9UrmLbxuysysqNCHksJ1wJQedYuBoyPiGODXwCVV7p8YEcdGxPg0jdUrmZuZtbx69swjYimwuUfdnWX7Wi2njrvPVk3mkhZKOjzFczzMYmYtr84981r+muIEk0oCuFPSfZLa0zysVs/82uSBcyUNqXLdpDSNmZk1s74kc0ntklaUlVRJl+K9cynuQHtTL5ecFBHHU1zPc66kP671zFqzWX4g6XaKy/lXSPouZf9Riogrk8/NvTzCzKxldCv9IENEdAAdfW1D0hkUtxWfFFF5n8aI6Ew+N0laAEwAllZ7bpox850UN9faj+JiofJiZpYbBZS69IekKcAngfdExI5ermmTNLT0HZgMrKx0bblaK0CnAFcCtwLH99a4mVke1HNanqT5wDuBgyWtBy6jOHtlP2Cxiv8vYHlEnCNpBHBNREwDhgMLkvP7ADdHxKJa7dVaAToX+MuIeLiffx4zs5ZRz+X8EVFpseV3ern2CWBa8n0t8Ja+tldrzPzkvj7QzKxVFfowZt5s0m6Ba2aWe628+tHJ3Mws0dW6HXMnczOzkv7OUmkGTuZmZgkPs5iZ5UChdTvmTuZmZiV5ftOQmdleo9s9czOz1ueeuZlZDjiZm5nlQAu/AtTJ3MysxD1zM7MccDI3M8sBz2YxM8sB98zNzHLAydzMLAe8N4uZWQ54bxYzsxzozjqAPeBkbmaWKLTwQMugrAMwM2sWhT6UWiTNk7RJ0sqyuoMkLZa0Jvkc1su9s5Jr1kialSZ2J3Mzs0T0oaRwHTClR93FwJKIGAMsSY5fRNJBwGXACcAE4LLekn45J3Mzs0Q9e+YRsRTY3KN6OnB98v164L0Vbv0zYHFEbI6IZ4HFvPQ/Ci/hZG5mligofZHULmlFWWlP0cTwiNiQfH8SGF7hmpHAurLj9UldVf4B1Mws0d2HH0AjogPo6G9bERGS6vaLq3vmZmaJeg6z9GKjpEMBks9NFa7pBEaXHY9K6qpyMjczSxSI1KWfbgVKs1NmAT+ucM0dwGRJw5IfPicndVU5mZuZJeo5m0XSfOBuYKyk9ZJmA1cAp0paA5ySHCNpvKRrACJiM/B54N6kXJ7UVeUxczOzRD032oqImb2cmlTh2hXAWWXH84B5fWnPydzMLNHKK0CdzM3MEt6bxcwsB8I9czOz1ueXU1hD/emZ0zhpxiQkseyWn7Fk3sKsQ7IMtJ0/hyHj305hy7NsveDM3fX7vet9vGzae4lCgV0rlvP89VdnGGVr85i5NcyIN4zmpBmT+PL0S+je1cUF18/lwSX389Rvn8w6NBtgv19yOy/89Ee0XfSp3XX7vPk49j3hRLZcOBu6dqFXHphdgDnQuqnc88yb3muOHMlvHniUXS/spNBd4Nf3/IrjpkzIOizLQNevHiSe2/aiuv2mTOf5H94MXbsAiC2/yyCy/OgiUpdm42Te5J54ZB1j3noUbQfuz5CX7cubJx7PQYcenHVY1iQGjxjFkHHHcMBXr2LoF7/J4COPyjqklhZ9+KfZZDLMIunMiLg2i7ZbzZOPdXLH1T/mwu9+hp07XmDdrx6nUGjln2msrgYPRvsfwNZPfJTBY45i/09+li3tM7KOqmW18v+ysuqZf663E+XbSq7atnYgY2pad33/53zp3XP42gcuY8eW59i49omsQ7ImUXjmKXYuXwpA95rVUCigA16ZcVStq5V75g1L5pIe7KU8ROU9fIHitpIRMT4ixr9x6BGNCq+lDH3VAQAMG3Ewx005gV/euizjiKxZ7LpnGUPefBwAg0aMgiFDiK1bMo6qdQ3ArokN08hhluEU35jxbI96Af/dwHZz5+yr/p62YUPp7upi/meu4fmtO7IOyTLQ9vFLGXL0seiAV3Lgd37AjvnX8vufLaTt/Dkc8K1roauL7d/4UtZhtrRCNF+PO61GJvPbgP0j4oGeJyT9RwPbzZ2vnX5p1iFYE9j+9csr1//jFwc4kvzqy8spmk3DknlEzK5y7oONatfMrL+acSw8LS8aMjNLNONYeFpO5mZmCS/nNzPLAQ+zmJnlgIdZzMxyoDtaN507mZuZJVo3lXujLTOz3eq1nF/SWEkPlJWtki7qcc07JW0pu2aPFpS4Z25mlqjXbJaIeAQ4FkDSYKATWFDh0v+KiNPq0aaTuZlZIhqznH8S8FhE/LYRDy/xMIuZWaKbSF3Kd3hNSnsvj50BzO/l3Nsl/a+k2yW9aU9id8/czCzRl2GWiOgAOqpdI2lf4D3AJRVO3w8cFhHPSZoG/DswJnUAPbhnbmaWiIjUJaWpwP0RsbFCW1sj4rnk+0JgiKR+v0bMPXMzs0QDlvPPpJchFkmvATZGREiaQLFz/Ux/G3IyNzNL1HM5v6Q24FTg7LK6cwAi4mrg/cBHJXUBzwMzYg9+gXUyNzNL1PPlFBGxHXhVj7qry75/G/h2vdpzMjczS/jlFGZmOeAtcM3McqBBi4YGhJO5mVnCPXMzsxzwyynMzHLAwyxmZjngl1OYmeWAx8zNzHLAY+ZmZjlQzxWgA83J3Mws4Z65mVkO+AdQM7Mc8DCLmVkOeJjFzCwH3DM3M8sB98zNzHIg/AOomVnr82wWM7Mc8HJ+M7McqOeuiZIeB7YB3UBXRIzvcV7AN4FpwA7gjIi4v7/tOZmbmSUaMJtlYkQ83cu5qcCYpJwAXJV89sug/t5oZpY30Yd/6mA6cEMULQcOlHRofx/mZG5mloiI1EVSu6QVZaW95+OAOyXdV+EcwEhgXdnx+qSuXzzMYmaW6MtslojoADqqXHJSRHRKOgRYLGl1RCzd0xh74565mVmiEJG61BIRncnnJmABMKHHJZ3A6LLjUUldvziZm5kl+jLMUo2kNklDS9+BycDKHpfdCnxERW8DtkTEhv7G7mEWM7NEHeeZDwcWFGcfsg9wc0QsknQOQERcDSykOC3xUYpTE8/ckwadzM3MEvWaZx4Ra4G3VKi/uux7AOfWpUGczM3MdvNyfjOzHPAWuGZmOVDP5fwDzcnczCzh/czNzHLAPXMzsxxo5WSuVg5+byKpPVk+bLab/15YiVeAto5KG/WY+e+FAU7mZma54GRuZpYDTuatw+OiVon/XhjgH0DNzHLBPXMzsxxwMjczywEn8xYgaYqkRyQ9KunirOOx7EmaJ2mTpJ4vPLC9lJN5k5M0GPhnYCowDpgpaVy2UVkTuA6YknUQ1jyczJvfBODRiFgbETuBW4DpGcdkGUteDLw56ziseTiZN7+RwLqy4/VJnZnZbk7mZmY54GTe/DqB0WXHo5I6M7PdnMyb373AGEmvk7QvMAO4NeOYzKzJOJk3uYjoAs4D7gBWAd+PiIezjcqyJmk+cDcwVtJ6SbOzjsmy5eX8ZmY54J65mVkOOJmbmeWAk7mZWQ44mZuZ5YCTuZlZDjiZm5nlgJO5DSgVLZM0tazuLyUtGoC23ynptka3Y5aFfbIOwPYuERGSzgF+IOkXFP8Ofok92M5V0uCI6K5XjGatyMncBlxErJT0E2AO0AbcEBGP9bxO0uHAIuA+4HjgYeAjEbFD0uPA94BTga9I2gx8DtgPeAw4MyKekzQF+AawA1jW4D+aWWY8zGJZ+RzwQYov3fhKlevGAv8SEW8EtgJ/W3bumYg4HvgZ8GnglOR4BfB3kl4G/BvwbuCPgNfU/U9h1iSczC0TEbGdYs/6uxHx+yqXrouIu5LvNwInlZ37XvL5NopvYbpL0gPALOAw4CjgNxGxJor7VtxYxz+CWVPxMItlqZCUanpuHlR+vD35FLA4ImaWXyjp2D2KzqyFuGduze61kt6efP8glce9lwMnSjoSQFKbpDcAq4HDJb0+uW5mhXvNcsHJ3JrdI8C5klYBw4Crel4QEU8BZwDzJT1IcWvYoyLiBaAd+Kmk+4FNAxa12QDzFrjWtJLZLLdFxNFZx2LW7NwzNzPLAffMLXOSXgUsqXBqUkQ8M9DxmLUiJ3MzsxzwMIuZWQ44mZuZ5YCTuZlZDjiZm5nlwP8DLL0j/uHkp9oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix(y_test,y_pred),annot=True)\n",
    "plt.xlabel('Y_pred')\n",
    "plt.ylabel('Y_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:14.946236Z",
     "start_time": "2020-10-17T06:29:14.932083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets use Dropout regularization to remove overfitting issue. This can be done by introducing dropout layers in between \n",
    "# the hidden layers. The general practice is also to introduce the dropout layers in between the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:30:04.923432Z",
     "start_time": "2020-10-17T06:29:57.683180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 156 samples\n",
      "Epoch 1/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.7396 - accuracy: 0.4679\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 307us/sample - loss: 0.7141 - accuracy: 0.4808\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 319us/sample - loss: 0.7131 - accuracy: 0.5513\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 332us/sample - loss: 0.7142 - accuracy: 0.5128\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 307us/sample - loss: 0.7350 - accuracy: 0.4295\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 313us/sample - loss: 0.6753 - accuracy: 0.5449\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 322us/sample - loss: 0.7044 - accuracy: 0.5256\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 322us/sample - loss: 0.7073 - accuracy: 0.5064\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 375us/sample - loss: 0.6982 - accuracy: 0.5577\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 339us/sample - loss: 0.6884 - accuracy: 0.5064\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 317us/sample - loss: 0.7026 - accuracy: 0.5641\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 595us/sample - loss: 0.7079 - accuracy: 0.4936\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 620us/sample - loss: 0.6864 - accuracy: 0.5128\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 543us/sample - loss: 0.6843 - accuracy: 0.5064\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 607us/sample - loss: 0.6964 - accuracy: 0.5256\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 441us/sample - loss: 0.6982 - accuracy: 0.5577\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 486us/sample - loss: 0.7033 - accuracy: 0.5064\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 531us/sample - loss: 0.7032 - accuracy: 0.4872\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 396us/sample - loss: 0.6853 - accuracy: 0.6154\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 326us/sample - loss: 0.6766 - accuracy: 0.5897\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 336us/sample - loss: 0.6814 - accuracy: 0.5128\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 384us/sample - loss: 0.6669 - accuracy: 0.5962\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 339us/sample - loss: 0.6682 - accuracy: 0.5449\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 332us/sample - loss: 0.6521 - accuracy: 0.5897\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 343us/sample - loss: 0.6526 - accuracy: 0.5769\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 326us/sample - loss: 0.6736 - accuracy: 0.6346\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 312us/sample - loss: 0.6559 - accuracy: 0.6346\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 320us/sample - loss: 0.6386 - accuracy: 0.5897\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 401us/sample - loss: 0.6458 - accuracy: 0.6282\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 339us/sample - loss: 0.6590 - accuracy: 0.6474\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 326us/sample - loss: 0.6049 - accuracy: 0.6987\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 351us/sample - loss: 0.6442 - accuracy: 0.5962\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 326us/sample - loss: 0.6418 - accuracy: 0.6218\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 329us/sample - loss: 0.6187 - accuracy: 0.6218\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 351us/sample - loss: 0.6248 - accuracy: 0.6538\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 313us/sample - loss: 0.6163 - accuracy: 0.6603\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 339us/sample - loss: 0.6502 - accuracy: 0.6090\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 328us/sample - loss: 0.6141 - accuracy: 0.6218\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 323us/sample - loss: 0.5753 - accuracy: 0.6731\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 325us/sample - loss: 0.5601 - accuracy: 0.7115\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 339us/sample - loss: 0.5929 - accuracy: 0.6667\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 337us/sample - loss: 0.5345 - accuracy: 0.7436\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 333us/sample - loss: 0.5899 - accuracy: 0.6923\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 339us/sample - loss: 0.5512 - accuracy: 0.7372\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 332us/sample - loss: 0.6140 - accuracy: 0.7115\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 333us/sample - loss: 0.5408 - accuracy: 0.7308\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 333us/sample - loss: 0.5377 - accuracy: 0.7628\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 409us/sample - loss: 0.5182 - accuracy: 0.7821\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 320us/sample - loss: 0.5465 - accuracy: 0.7179\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 334us/sample - loss: 0.5074 - accuracy: 0.7564\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 384us/sample - loss: 0.5360 - accuracy: 0.7244\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 391us/sample - loss: 0.5955 - accuracy: 0.6859\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 332us/sample - loss: 0.5263 - accuracy: 0.7308\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 300us/sample - loss: 0.5471 - accuracy: 0.7308\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 300us/sample - loss: 0.5273 - accuracy: 0.7115\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 300us/sample - loss: 0.5289 - accuracy: 0.7372\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 300us/sample - loss: 0.5176 - accuracy: 0.7756\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 305us/sample - loss: 0.5021 - accuracy: 0.7692\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 307us/sample - loss: 0.4437 - accuracy: 0.8141\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 306us/sample - loss: 0.5226 - accuracy: 0.7500\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 307us/sample - loss: 0.4974 - accuracy: 0.7628\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 300us/sample - loss: 0.5280 - accuracy: 0.7692\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 305us/sample - loss: 0.4273 - accuracy: 0.8013\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 307us/sample - loss: 0.4528 - accuracy: 0.8077\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 333us/sample - loss: 0.5009 - accuracy: 0.7628\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 298us/sample - loss: 0.4959 - accuracy: 0.7885\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 397us/sample - loss: 0.4257 - accuracy: 0.8013\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 326us/sample - loss: 0.4568 - accuracy: 0.7756\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 326us/sample - loss: 0.4865 - accuracy: 0.8013\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 301us/sample - loss: 0.4925 - accuracy: 0.7821\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 326us/sample - loss: 0.4720 - accuracy: 0.7821\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 307us/sample - loss: 0.4378 - accuracy: 0.8141\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 314us/sample - loss: 0.5257 - accuracy: 0.8269\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 304us/sample - loss: 0.4196 - accuracy: 0.8141\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 325us/sample - loss: 0.4042 - accuracy: 0.8269\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 330us/sample - loss: 0.4118 - accuracy: 0.8205\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 313us/sample - loss: 0.3527 - accuracy: 0.8397\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 291us/sample - loss: 0.4597 - accuracy: 0.7756\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 306us/sample - loss: 0.4553 - accuracy: 0.7821\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 319us/sample - loss: 0.3854 - accuracy: 0.8013\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 291us/sample - loss: 0.3671 - accuracy: 0.8590\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 313us/sample - loss: 0.3883 - accuracy: 0.8077\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 307us/sample - loss: 0.4094 - accuracy: 0.8141\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 319us/sample - loss: 0.3742 - accuracy: 0.8333\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 307us/sample - loss: 0.4202 - accuracy: 0.8141\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 299us/sample - loss: 0.3927 - accuracy: 0.8141\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 384us/sample - loss: 0.4244 - accuracy: 0.8333\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 371us/sample - loss: 0.3649 - accuracy: 0.8654\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 314us/sample - loss: 0.3399 - accuracy: 0.8333\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 288us/sample - loss: 0.3349 - accuracy: 0.8718\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 299us/sample - loss: 0.3647 - accuracy: 0.8718\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 313us/sample - loss: 0.3095 - accuracy: 0.8910\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 322us/sample - loss: 0.3464 - accuracy: 0.8846\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 313us/sample - loss: 0.3294 - accuracy: 0.8782\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 281us/sample - loss: 0.3870 - accuracy: 0.8269\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 320us/sample - loss: 0.4066 - accuracy: 0.8718\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 294us/sample - loss: 0.2905 - accuracy: 0.8590\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 319us/sample - loss: 0.4209 - accuracy: 0.8077\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 304us/sample - loss: 0.3520 - accuracy: 0.8462\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 320us/sample - loss: 0.3145 - accuracy: 0.8718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1de177a4e08>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeld=keras.Sequential([\n",
    "    keras.layers.Dense(60,input_dim=60,activation='relu'), # We can use input_dim instead of input_shape=(n,). Input to hidden\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(30,activation='relu'), # Hidden to Hidden\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(15,activation='relu'), # Hidden to Hidden\n",
    "    keras.layers.Dropout(0.5),                # You can also change the dropout rate for each layer.\n",
    "    keras.layers.Dense(1,activation='sigmoid') # Hidden to Output\n",
    "])\n",
    "modeld.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "modeld.fit(X_train,y_train, epochs=100, batch_size=8) # We specified batch_size because we are performing kind off mini batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:30:05.077929Z",
     "start_time": "2020-10-17T06:30:04.927484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "52/52 [==============================] - 0s 866us/sample - loss: 0.4307 - accuracy: 0.7885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.43074912749803984, 0.78846157]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeld.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:29:21.610853Z",
     "start_time": "2020-10-17T06:29:21.596109Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can see that our accuracy has been increased and also now our model is not overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:31:02.068687Z",
     "start_time": "2020-10-17T06:31:01.982316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "y_predd=modeld.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:31:49.198784Z",
     "start_time": "2020-10-17T06:31:49.193788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0.], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predd=y_predd.reshape(52)\n",
    "y_predd=y_predd.round()\n",
    "y_predd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:32:03.104676Z",
     "start_time": "2020-10-17T06:32:03.095705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81        27\n",
      "           1       0.82      0.72      0.77        25\n",
      "\n",
      "    accuracy                           0.79        52\n",
      "   macro avg       0.79      0.79      0.79        52\n",
      "weighted avg       0.79      0.79      0.79        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_predd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T06:35:34.783742Z",
     "start_time": "2020-10-17T06:35:34.766611Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can see the improvement of the f1-score in this case. Dropout Regularization does not guarantee you that the model\n",
    "# is improved but your model is likely to be improved. This is because of the random dropping of the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout regularization is used in the case of computer vision or related problems where your neural networks are quite dense."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
