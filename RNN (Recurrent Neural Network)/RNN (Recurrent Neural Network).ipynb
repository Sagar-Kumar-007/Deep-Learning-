{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***RNN:***<br>\n",
    "RNN works very well with sequence of data as your input. Ex: classifying sentences as positive or negative, spam classifier, time-series data, sales forecasting, stocks forecasting etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While using BOW (bag Of Words) in NLP like Count Vectorizer, TF-IDF and Word2Vec (from Gensim), we can see that the words do not follow a sequence and hence when the words are combined to form a sentence, there is no guarantee that the sentence will make a meaning. The sequence information is discarded when it is converted into vectors. Ex: My name is Sagar Kumar. This sentence is in a sequence and hence it makes some meaning. If the sentence was not in a sequence or if it was jumbled there would not be any meaning to the sentence. Once the sequence information is discarded the accuracy would go towards a lower side. This sequence information plays a vital role in chatbots like amazon's alexa, google assistant etc. To control this sequential information, we use RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use RNN to build a model for the time-series data like stocks forecasting, sales prediction etc. RNN also gives comparatively larger accuracy when compared with ARIMA and SARIMAX which are specially used to predict sales and stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN is also used in the cases of Google Image search and Image Captioning. Ex: You will get google image results if you search for a man running behind a dog (note that it is a sentence) and also if you give an image of the man running behind a dog to an RNN model, it would caption the whole sentence for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Translator is a very good example of RNN. Here you give a sentence in English and then translate it into French. You would have known by now that the sequence of information is very much important and RNN can be used to store it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Working:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Forward Propagation with Time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='rnn.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above image is the general architecture of an RNN. In this, 'X' is the input and the input can be of any number of dimensions/features. The 'h' is the hidden layer and here you can have any number of hidden neurons. The 'y' is the output. Along with this output, we get output with respect to time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='a.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a sentence of 4 words like \"I am Sagar Kumar\". Split the words into Xi1,Xi2,Xi3 and Xi4. While forward Propagation in RNN, you first pass on the first word to the Neural Network having some number of neurons and inititalize weight 'W' to it. Then you calculate the output (0 or 1 if you consider sentiment analysis) and store it in 'O1'. Output can be a function that looks like f(Xi1 * W).Now you pass the second word 'Xi2' into the same neural network with same weight 'W' (because you are using the same neural network and you haven't back propagated it yet to adjust the weights). Now, you consider the second word 'Xi2' and the previous output 'O1' as the input to your neural network. The weight given to O1 is \" W' \" (say). Now you train your model again. Your output will be a function f((Xi2 * W) + (O1 * W')). You repeat the same process until you have used all your words to train the model. This is Forward Propagation in RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back Propagation works similar like any other neural network. After each back propagation, the weights and the bias is adjusted based on the loss function. New weights/bias is the difference between the old weights/bias and the multiplication of learning rate and the derivative of the loss function w.r.t the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problems faced while using simple RNN:***<br>\n",
    "1. Exploding Gradient Problem\n",
    "2. Vanishing Gradient Problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Rectify these issues we are introduced with the concept known as LSTM (Long Short Term Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
