{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Why activation functions are important:***<br>\n",
    "Take an example of a dataset where you want to predict whether a person buys an insurance or not. Considering the independent variables to be age, education and income and taking its weighted sum (i.e., regression line) might not be a good idea to predict the status of the person because its value may range between -ve infinity to +ve infinity. In order to predict the status we might be considering to use the probability based on the values. Hence we use a sigmoid function to convert the value between 0 to 1. Here, the sigmoid function does the role of an activation function.<br>\n",
    "You can see that *having a sigmoid function helps to predict the value in the output layers.* What about the hidden layers?\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a neural network <img src='nn.jpg' width=400><br>\n",
    "You may observe that the neurons of the neural network has a weighted sum and an activation function. Lets remove the activation functions from the hidden layers and the output layer.<br><img src='nn_no_af.jpg' width=400><br>\n",
    "If you do the math you may see that the result would give you a weighted sum or simply a linear equation of all the independent variables. But this will not always give you accurate results.We also know that the many patterns do not follow linearly and hence non-linear patterns can be required. Hence, Activation functions are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Activation Functions:***\n",
    "<br>\n",
    "1. ***Step Function vs Sigmoid Function***:\n",
    "<br><img src='step.jpg' width=500><br>Since step function values either be the 1 or 0 depending whether the values are greater than 0.5 or less than 0.5, we can use the step function to classify the independent variables and predict its outcome. Step function is not a very popular activation function because its values are constrained to be either 0 or 1 with a slope of 90 degrees i.e., there is a sudden change between 0 and 1 rather than a gradual change like that of a sigmoid function.There is another problem that is faced while using a step function is that it may predict more than one output. For example, suppose that you are predicting hand written digits. Since step function only outputs either 0 or 1, there are chances where you may get the output for the digit (say 4) as 2 and 4. As there are multiple outputs for the same variable, it becomes difficult for us to classify them and fire the correct neuron (finalize the neuron to be used). If you use sigmoid function in this case, you may get value between 0 and 1 instead of 0 or 1 (due to the gradual change between 0 and 1 unlike step function) and then select the neuron with the highest probability. The values in the sigmoid function is of float and hence there is unlikely any chance that two values may match.<br><img src='sigmoid.jpg' width=500><br>\n",
    "2. ***tan(h) function vs Sigmoid:***<br>\n",
    "There is another function tan(h) that also looks like a sigmoid function but its value ranges between -1 to +1.<br><img src='tanh.jpg' width=500><br>***Use Sigmoid in the output layer and try to use tan(h) function in all the other layers i.e., hidden layers.***<br>You may have understood that sigmoid function helps in classification and hence it can be used to predict the output. While tan(h) can be used in other cases because of its higher range of values. tan(h) is always better than sigmoid function because it changes the values to -1 and +1 which rises to give a mean of zero which centres the data. This may lead to symmetry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Issues with tan(h) and sigmoid:***\n",
    "<br>\n",
    "In both sigmoid and tan(h) functions calculating the change of the output (whether a person buys insurance or not, for example) with respect to input (say age) with higher values of input will be very low (because slope will be zero) and hence the learning rate will be very slow. The learning becomes slow because, while traiing the model, we need to calculate the slope and if there is an error, we need to back propagate which becomes extremely slow because of the slope being near to zero (Gradient Descent Concept). This is called Vanishing Gradients problem.<br> ***Note that both sigmoid and tan(h) functions has Vanishing Gradient Problem. Hence, learning process is very slow while using sigmoid and tan(h) functions.***<br>To resolve this issue we came up with an activation function called ***ReLu.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ReLu:***\n",
    "<br>\n",
    "The function states that if the input is less than zero then the value is zero and if the input is greater than zero then the output is same as that of input.<br><img src='relu.jpg' width=500><br>\n",
    "***For hidden layers, if you are not sure which activation function to use, just use ReLu as your default choice.***<br>\n",
    "ReLu also has vanishing gradient problem when the input is less than zero. To resolve this, there is another type of ReLu called ***Leaky ReLu.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Leaky ReLu:***<br>\n",
    "<img src='leaky_relu.jpg' width=500><br>\n",
    "Leaky ReLu is also used based on the circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In Short, if you are trying to find output for a binary classification, you may use Sigmoid in the output layer and for the hidden layers you may try using ReLu and leaky ReLu as your default choice.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
